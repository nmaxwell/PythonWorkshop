\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, enumerate, mathenv}
\usepackage[arrow,matrix,curve,cmtip,ps]{xy}
\usepackage{graphicx, float}
%\usepackage{enumitem}



%Set margins:

\setlength{\topmargin}{-1in}
\setlength{\textheight}{10in}
\setlength{\oddsidemargin}{-0.5in}
\setlength{\textwidth}{7.3in}

%Macros:

\newcommand{\pset}[1]{ \mathcal{P}(#1) }
\newcommand{\st}[0]{ \textrm{ s.t. } }
\newcommand{\wrt}[0] { \textrm{ w.r.t. } }
\newcommand{\inv}[0] { ^{-1}}
%\newcommand{ \cf }[1] { \chi_{_{#1}} }
\newcommand{ \cf }[1] { \mathbf{1}_{#1} }


\newcommand{ \supp } { \textrm{supp} }

\newcommand{\rimply}[0] { \Rightarrow }
\newcommand{\limply}[0] { \Leftarrow }
\newcommand{\rlimply}[0] { \Leftrightarrow }
\newcommand{\lrimply}[0] { \Leftrightarrow }

\renewcommand{\Re}{ \operatorname{Re} }
\renewcommand{\Im}{ \operatorname{Im} }

\newcommand{\nats}[0] { \mathbb{N}}
\newcommand{\reals}[0] { \mathbb{R}}
\newcommand{\scalars}[0] { \mathbb{F}}
\newcommand{\Cdb}[0] { \mathbb{C}}
\newcommand{\Rdb}[0] { \mathbb{R}}
\newcommand{\Fdb}[0] { \mathbb{F}}
\newcommand{\rationals}[0] { \mathbb{Q}}
\newcommand{\complexes}[0] { \mathbb{Q}}
\newcommand{\cmplxs}[0] { \mathbb{Q}}
\newcommand{\ints}[0] { \mathbb{Z}}


\newcommand{\eps}[0] {  \epsilon }
\newcommand{\lam}[0] {  \lambda }
\newcommand{\Lam}[0] {  \Lambda }
\newcommand{\om}[0] { \omega }
\newcommand{\Om}[0] { \Omega }

\newcommand{\A}[0] { \mathcal{A} }
\newcommand{\B}[0] { \mathcal{B} }
\newcommand{\C}[0] { \mathcal{C} }
\newcommand{\D}[0] { \mathcal{D} }
\newcommand{\E}[0] { \mathcal{E} }
\newcommand{\F}[0] { \mathcal{F} }
\newcommand{\G}[0] { \mathcal{G} }
\newcommand{\M}[0] { \mathcal{M} }
\newcommand{\N}[0] { \mathcal{N} }
\newcommand{\curlyO}[0] { \mathcal{O} }
\newcommand{\curlyP}[0] { \mathcal{P} }
\newcommand{\R}[0] { \mathcal{R} }
\newcommand{\curlyS}[0] { \mathcal{S} }
\newcommand{\U}[0] { \mathcal{U} }
\newcommand{\V}[0] { \mathcal{V} }
\newcommand{\W}[0] { \mathcal{W} }
\newcommand{\Bl}[0] { \mathcal{B} \ell }
\newcommand{\Ell}[0] { \mathcal{L} }

%crap


\newcommand{\fall}[0] { \text{ for all } }
\newcommand{\Var}[0] { \text{ Var } }
\newcommand{\defeq}[0] { := }
\newcommand{\Ball}[0] {\text{Ball}}
\newcommand{\partset}[1]{ \mathcal{P}^{*}(#1) }
\newcommand{\aew}[0] { \text{ a.e. } }
\newcommand{\where}[0] { \textrm{ where } }
\newcommand{\norm}[1] { \| #1 \| }
\newcommand{\Ran}[0] { \text{Range} }
\newcommand{\OR}[0] { \text{ or } }
\newcommand{\AND}[0] { \text{ and } }
\newcommand{\RP}[0] { \mathbb{RP} }
\newcommand{\Trace}[0] { \text{Trace} }
%\newcommand{\Var}[0] { \text{Var} }

\newcommand{\parder}[2] {\frac{\partial #1 }{ \partial #2}}





\begin{document}

\begin{flushleft}
{\bf Statistics, M6383,  Homework 5: Nicholas Maxwell 5/2/2011}
\end{flushleft}


Program description: \\

construct $p$ as in HW5; $p: \reals^10 \to \reals$ a polynomial, $p(x) = \sum_{\alpha \in \nats^n, |\alpha| \le 3} c_\alpha x^\alpha$, where $\alpha$ is a multi-index. The coefficients $c_i$ are chosen randomly $\sim N(0,1)$, and fixed throughout. \\

For $1 \le i \le 10000$, chose $x_i \in \reals^10$ randomly, $x_{i,j} \sim N(\mu_j, \frac{1}{2} \mu_j)$, where $\mu_j$ are chosen randomly and fixed at the beginning, $\mu_j \sim N(0,1)$. \\

Let ${\bf x} = (x_j)$, then $MM = {\bf x x^T}$, then $D_{i,j} = MM_{i,i} + MM_{j,j} - 2 MM_{i,j}$, so $D_{i,j} = \| x_i - x_j\|^2$. Compute this matrix before hand and save, this is program HW5\_1.py \\

In HW5\_2.py, load these data. Define the function, fit, which takes n\_samples as a parameter, and $\sigma, \lambda$. construct the matrix $M_{i,j} = \exp(-D_{i,j}/(2 \sigma^2))$. Then construct the fit by $\hat \beta = (M^T M + \lambda^2 I) \inv M^T Y$. Then return the relative $\ell_2$ error between $Y$ and $M \hat \beta$. \\ 

Define a training function which picks values of $\sigma, \lambda$, randomly, according to a Weibull distribution, where the shape parameter is 1.0, and the scale parameter is chosen so that the median of the distribution is 1.0. Then choose the value of $\sigma, \lambda$ which yields the smallest error from the function, fit, defined previously. This yields values of $\sigma, \lambda$, which achieve fairly small error. \\

Next, using these values of $\sigma, \lambda$, apply a standard minimization routine, which optimizes $\sigma, \lambda$, with the cost function, fit($\sigma, \lambda$), i.e. the relative error in the fit, given $\sigma, \lambda$. Now this yields final values of $\sigma, \lambda$ which give very accurate results, on the training data. \\

Generate test data, by drawing new samples, ${\bf x}$, and using the optimized $\sigma, \lambda$, print error. 




\end{document}







