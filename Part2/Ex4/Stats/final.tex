\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, enumerate, mathenv}
\usepackage[arrow,matrix,curve,cmtip,ps]{xy}
\usepackage{graphicx, float}
%\usepackage{enumitem}



%Set margins:

\setlength{\topmargin}{-1in}
\setlength{\textheight}{10in}
\setlength{\oddsidemargin}{-0.5in}
\setlength{\textwidth}{7.3in}

%Macros:

\newcommand{\pset}[1]{ \mathcal{P}(#1) }
\newcommand{\st}[0]{ \textrm{ s.t. } }
\newcommand{\wrt}[0] { \textrm{ w.r.t. } }
\newcommand{\inv}[0] { ^{-1}}
%\newcommand{ \cf }[1] { \chi_{_{#1}} }
\newcommand{ \cf }[1] { \mathbf{1}_{#1} }


\newcommand{ \supp } { \textrm{supp} }

\newcommand{\rimply}[0] { \Rightarrow }
\newcommand{\limply}[0] { \Leftarrow }
\newcommand{\rlimply}[0] { \Leftrightarrow }
\newcommand{\lrimply}[0] { \Leftrightarrow }

\renewcommand{\Re}{ \operatorname{Re} }
\renewcommand{\Im}{ \operatorname{Im} }

\newcommand{\nats}[0] { \mathbb{N}}
\newcommand{\reals}[0] { \mathbb{R}}
\newcommand{\scalars}[0] { \mathbb{F}}
\newcommand{\Cdb}[0] { \mathbb{C}}
\newcommand{\Rdb}[0] { \mathbb{R}}
\newcommand{\Fdb}[0] { \mathbb{F}}
\newcommand{\rationals}[0] { \mathbb{Q}}
\newcommand{\complexes}[0] { \mathbb{Q}}
\newcommand{\cmplxs}[0] { \mathbb{Q}}
\newcommand{\ints}[0] { \mathbb{Z}}


\newcommand{\eps}[0] {  \epsilon }
\newcommand{\lam}[0] {  \lambda }
\newcommand{\Lam}[0] {  \Lambda }
\newcommand{\om}[0] { \omega }
\newcommand{\Om}[0] { \Omega }

\newcommand{\A}[0] { \mathcal{A} }
\newcommand{\B}[0] { \mathcal{B} }
\newcommand{\C}[0] { \mathcal{C} }
\newcommand{\D}[0] { \mathcal{D} }
\newcommand{\E}[0] { \mathcal{E} }
\newcommand{\F}[0] { \mathcal{F} }
\newcommand{\G}[0] { \mathcal{G} }
\newcommand{\M}[0] { \mathcal{M} }
\newcommand{\N}[0] { \mathcal{N} }
\newcommand{\curlyO}[0] { \mathcal{O} }
\newcommand{\curlyP}[0] { \mathcal{P} }
\newcommand{\R}[0] { \mathcal{R} }
\newcommand{\curlyS}[0] { \mathcal{S} }
\newcommand{\U}[0] { \mathcal{U} }
\newcommand{\V}[0] { \mathcal{V} }
\newcommand{\W}[0] { \mathcal{W} }
\newcommand{\Bl}[0] { \mathcal{B} \ell }
\newcommand{\Ell}[0] { \mathcal{L} }

%crap


\newcommand{\fall}[0] { \text{ for all } }
\newcommand{\Var}[0] { \text{ Var } }
\newcommand{\defeq}[0] { := }
\newcommand{\Ball}[0] {\text{Ball}}
\newcommand{\partset}[1]{ \mathcal{P}^{*}(#1) }
\newcommand{\aew}[0] { \text{ a.e. } }
\newcommand{\where}[0] { \textrm{ where } }
\newcommand{\norm}[1] { \| #1 \| }
\newcommand{\Ran}[0] { \text{Range} }
\newcommand{\OR}[0] { \text{ or } }
\newcommand{\AND}[0] { \text{ and } }
\newcommand{\RP}[0] { \mathbb{RP} }
\newcommand{\Trace}[0] { \text{Trace} }
%\newcommand{\Var}[0] { \text{Var} }

\newcommand{\parder}[2] {\frac{\partial #1 }{ \partial #2}}





\begin{document}

\begin{flushleft}
{\bf Statistics, M6383,  Final Project: Nicholas Maxwell 5/12/2011}
\end{flushleft}

I wrote a script which automatically retrieves stock market data from ``http://www.google.com/finance'', given a ticker symbol and start, end dates. The ticker symbols that I used were: \\

NASDAQ:INTC, NYSE:IBM, NASDAQ:MSFT, NYSE:HPQ, NYSE:F, NYSE:XOM, NASDAQ:ORCL, NYSE:PG, NYSE:JNJ, NYSE:SNY, NYSE:HMC, NYSE:FDX, NYSE:UPS, NYSE:KFT, NASDAQ:COKE, NYSE:PEP \\

I ran the program twice, once with data ranging from July 2002 through May 2011, and once with data ranging from January 2008 through May 2011. In the first case 2040 days of data were available, and in the second case, 853 days of data were available. 16 stocks were considered. \\


{\bf Part A: fitting.} The idea for part A is that one can reasonable expect that some of the market forces that affect a stock will be common to others, so that knowing the historical prices of some stocks, one might predict the value of another stock. \\

I used the stock of Intel Corporation (NASDAQ:INTC), as my $Y$ data, and used the remaining stocks from the above list as the $X_1, ... X_p$ data. First the usual linear least squares was used, 4.8\% relative $\ell_2$ error was obtained for the smaller data set, and 11\% error for the larger. \\

Next, I used the non-linear regression method to do the same thing. Now the data is randomly divided into a training set (1/2) and a testing set (1/2). Given a $\sigma, \lambda$, an $\alpha$ is determined by the Tikhonov regularization technique described in class. I picked 25 values of $\sigma, \lambda$ randomly (from a Weibull distribution), then picked the values which obtained the smallest (relative $\ell_2$ error on the training set + relative $\ell_2$ error and on the testing set). Then I used a standard optimization function to minimize this same error, by varying $\sigma, \lambda$, using the randomly determined value as a starting point. Once the optimal $\sigma, \lambda$ is determined, I compute the corresponding $\alpha$, then use that $\alpha$ to fit the data over the entire data set. The result was 2.2\% relative $\ell_2$ error for the smaller data set, and 2.1\% relative $\ell_2$  error for the larger data set. \\

{\bf Part B: distributions.} The idea for part B was to use the data from part A, but now compute the daily change in stock price, done by simply shifting the data one day, and subtracting the result. I fit four distributions to this data, the Normal, Laplace, Cauchy, and Hyperbolic secant distributions. I plot the histograms and fitted pdf, and include the resulting p-value from the Kolmogorov-Smirnov test, in the title of the plots. A p-value greater than 0.05 means the distribution function fits the data well, a value less that 0.05 means the fit is poor. \\

I found that the normal distribution was a very poor fit to all the stocks, the Cauchy distribution did a bit better, but the hyperbolic secant and Laplace distributions were excellent fits to the data. \\

For the smaller data set: normal passed one stock out of 16, Cauchy passed 3 of 16, hyperbolic sec passed 15 of 16, and Laplace passed all 16 stocks. \\

For the larger data set: normal failed all stocks, Cauchy failed all stocks, hyperbolic sec passed 13 of 16, and Laplace passed 13 of 16 stocks. Interestingly, Laplace and hyperbolic sec failed on different stocks, so together they passed all the stocks. \\





\end{document}







